# Learning Missingness Mechanisms for Imputation with Denoising Autoencoders

*Bachelor Thesis Project by Henry Heppe*

In this project a modified Denoising Autoencoder is proposed which can improve imputation performance. The imputation model consists of one DAE for which the training corruption process is generated by another model which is trained on partially observed data to replicate the underlying missingness mechanism. The full study is available at [EUR Thesis Repository](https://thesis.eur.nl/).

### Abstract  
Deep learning-based models for missing value imputation can often outperform well known statistical imputation methods such as regression-based imputation. Strong results can be achieved by using Denoising Autoencoders (DAE). These are neural networks that learn to reconstruct an observation from a corrupted version of itself. They effectively learn to reverse the corruption process that is artificially applied to the observation. Previous works use DAE for imputation with a manually specified corruption process. In this study, we propose the imputeLM imputation method. It is based on the idea of learning a model on the missingness mechanism itself. This model can then produce corruptions for the training of the DAE, which are more similar to the missingness patterns that one is imputing. Therefore, the DAElearns to reverse a closer representation of the missingness mechanism, which results in a theoretically more accurate imputation. We evaluate this hypothesis across differentmissingness scenarios for the MNIST, FashionMNIST and CIFAR10 datasets and find that incorporating learned missingness in the imputation process can be beneficial for difficult imputation tasks. Additionally, we also show that conditioning the corruption process on higher-level representations of the data can help generate more realistic missingness patterns.

### Structure of the Repository  
The neural networks are implemented in PyTorch. The repository is structured in the following way. It has one main notebook `imputation_models.ipynb`. This notebook brings together all parts of dataset generation, model training, testing and visualization, making use of all the `helper_*.py` files. This notebook can be used to do manual experiments with the model. It contains the implementation of the *imputeLM* model as well as the DAE, VAE and Mean imputation benchmark models. At the top of the notebook, a dictionary is included, which specifies all the data parameters (which dataset and which missingness parameters to use). Then for each model, there are another two dictionaries that specify the model configuration parameters and the model training. The `experiment_*.py` files can be used to reproduce the results included in the thesis.
