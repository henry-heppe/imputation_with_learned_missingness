import os
os.environ["CUBLAS_WORKSPACE_CONFIG"]=":4096:8"

import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.utils.data
import pandas as pd
import numpy as np
from torch.optim.lr_scheduler import StepLR
import importlib
from sklearn.impute import SimpleImputer
import utils
import data
import modelMP
import modelSDAE
import helper_train_MP
import helper_train_SDAE
import helper_train_MP_GAN
import helper_noise

importlib.reload(utils)
importlib.reload(data)
importlib.reload(modelMP)
importlib.reload(helper_train_MP)
importlib.reload(modelSDAE)
importlib.reload(helper_train_SDAE)
importlib.reload(helper_noise)
importlib.reload(helper_train_MP_GAN)

if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True)
torch.random.manual_seed(1)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

# randomness for the training of the Benchmark DAE
def dae_noise(x):
    return torch.rand_like(x)

# some overall experiment parameters
start_seed = 0
n_seeds = 10
mse_all = torch.zeros(4, 12, n_seeds)
accuracy_all = torch.zeros(4, 12, n_seeds)
dataset = 'CIFAR10'
folder_path = f'results/full_model_{dataset}'

os.makedirs(folder_path, exist_ok=True)

for cv_iteration in range(start_seed, start_seed+n_seeds):
    iteration = -1
    for noise_mechanism in ['patch', 'patches', 'mar', 'mnar', 'mcar', 'special_mnar_quant']:
        print(noise_mechanism)
        for noise_level_category in ['a', 'b']:
            iteration += 1
            
            if noise_mechanism in ['patch', 'patches']:
                noise_level = 0.1 if noise_level_category == 'a' else 0.2
            else:
                noise_level = 0.1 if noise_level_category == 'a' else 0.2
            sigmoid_offset = 0.15 if noise_level_category == 'a' else 0.05
            p_params = 0.4 if noise_level_category == 'a' else 0.7

            sdae_epoch = 1 if iteration < 4 else 10
            bdae_epoch = 1 if iteration < 4 else 10
            bvae_epoch = 1 if iteration not in [8, 9, 10] else 4

            torch.random.manual_seed(1+cv_iteration)
            dcon = {
                'dataset': dataset, # one of 'MNIST', 'FashionMNIST', 'CIFAR10'
                'noise_mechanism': noise_mechanism, # missingness mechanism ('mcar', 'mar', 'mnar', 'patch', 'patches', 'threshold', 'special_mar', 'special_mnar_log', 'special_mnar_self_log', 'special_mnar_quant')
                'na_obs_percentage': 0.4, # the number of observations that have missing values
                'replacement': 'uniform', # what value is plugged in for missing values in the observations with missing values (number or 'uniform')
                'noise_level': noise_level, # the percentage of missing values per observation (share of features that are missing)
                'download': False,
                'regenerate': True,
                'device': device,


                # for noise_mechanism 'patches'
                'patch_size_axis': 5, # size of the patch in the x and y direction

                # for noise_mechanism MAR and MNAR
                'randperm_cols': False, # whether to shuffle the columns of the data matrix before applying the noise mechanism
                'average_missing_rates': 'normal', # can be 'uniform', 'normal' or a list/tuple/tensor of length no. of features, determines how the missingness rates are generated
                # if uniform, noise_level is used as mean for uniform distribution and impossible values are clipped to the interval [0,1] --> noise_level != exact missingness rate in mask
                # if normal, noise_level is ignored and the options below apply
                'chol_eps': 1e-6, # epsilon for the cholesky decomposition (epsilon*I is added to the covariance matrix to make it positive definite)
                'sigmoid_offset': sigmoid_offset, # offset for the sigmoid function applied to the average missing rates generated by MultivariateNormal
                'sigmoid_k': 10, # steepness of sigmoid

                # for MNAR only
                'dependence': 'simple_unobserved', # can be 'simple_unobserved', 'complex_unobserved', 'unobserved_and_observed'

                ### for the 'special_*' missingness generators only
                # all of them
                'p': 0.8,

                # special_mar
                'p_obs': 0.6,

                # special_mnar_log
                # p as a before
                'p_params': p_params,
                'exclude_inputs': False,

                # special_mnar_quant
                # p and p_params as before
                'q': 0.2,
                'cut': 'both', # can be 'upper', 'lower', 'both'
                'MCAR': False, # whether MCAR is added on the non-MNAR mask
            }
            torch.random.manual_seed(1+cv_iteration)
            data_without_nas_1 = data.ImputationDatasetGen(config=dcon, missing_vals=False)
            data_with_nas_1 = data.ImputationDatasetGen(config=dcon, missing_vals=True)

            ## Encoder model
            mcon0 = {
                'architecture': 'encoder_model_dae',
                'loss': 'full', # must be full otherwise error
                'epochs': 10,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 700],
                'layer_dims_dec': [700, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': dae_noise,
                'corruption_share': noise_level, # level of the corruption noise that is used for training the DAE, here adjusted to actual noise level as that is also available in practice
                'mask_between_epochs': 'equal', # equal or random, determines the scope of the random generator that is passed to the mask bernoulli sampling function
                'additional_noise': 0, # does not apply here
            }

            tcon0 = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 4, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }

            if 'MNIST' in dcon['dataset']:
                mcon0['layer_dims_enc'][0] = 784
                mcon0['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon['dataset']:
                mcon0['layer_dims_enc'][0] = 1024
                mcon0['layer_dims_dec'][-1] = 1024

                
            nona_train_loader_0 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'train', tcon0['train_val_test_split']), batch_size=mcon0['batch_size'], shuffle=True)
            nona_val_loader_0 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'validation', tcon0['train_val_test_split']), batch_size=mcon0['batch_size'], shuffle=False)

            model_autoencoder = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=dae_noise, layer_dims_enc=mcon0['layer_dims_enc'], layer_dims_dec=mcon0['layer_dims_dec'], relu=mcon0['relu'], image=mcon0['image']).to(device)
            loss_fn_autoencoder = nn.MSELoss(reduction='none')
            optimizer_autoencoder = torch.optim.Adam(model_autoencoder.parameters(), lr=mcon0['learning_rate'])
            scheduler_autoencoder = StepLR(optimizer_autoencoder, step_size=mcon0['step_size'], gamma=mcon0['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_autoencoder, encoder=None, loss_fn=loss_fn_autoencoder, optimizer=optimizer_autoencoder, scheduler=scheduler_autoencoder,
                                                dcon=dcon, mcon=mcon0, tcon=tcon0,
                                                train_dataloader=nona_train_loader_0, validation_dataloader=nona_val_loader_0,
                                                noise_model=dae_noise)
            ## MP model
            model_autoencoder.eval()
            mcon = {
                'architecture': 'mask_pred_mlp',
                'epochs': 10,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims': [784, 2000, 2000, 2000, 784],
                'dropout': 0.5,
                'device': device,
                'relu': True,
                'image': True,
                'encoder': str(model_autoencoder)
                
            }

            tcon = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 4, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }

            if 'MNIST' in dcon['dataset']:
                mcon['layer_dims'][0] = mcon0['layer_dims_enc'][-1]
                mcon['layer_dims'][-1] = 784

            elif 'CIFAR10' in dcon['dataset']:
                mcon['layer_dims'][0] = mcon0['layer_dims_enc'][-1]
                mcon['layer_dims'][-1] = 1024



            nona_train_loader_1 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'train', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=True)
            nona_val_loader_1 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'validation', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)
            nona_test_loader_1 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'test', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)

            na_train_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'train', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=True)
            na_val_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'validation', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)
            na_test_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'test', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)

            model_mp = modelMP.MaskPredMLP(layer_dims=mcon['layer_dims'], dropout=mcon['dropout'], relu=mcon['relu'], image=mcon['image']).to(device)

            loss_fn = nn.BCELoss()
            optimizer = torch.optim.Adam(model_mp.parameters(), lr=mcon['learning_rate'])
            scheduler = StepLR(optimizer, step_size=mcon['step_size'], gamma=mcon['gamma'])

            helper_train_MP.train_model(model=model_mp, encoder=model_autoencoder.encoder, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler,
                                                dcon=dcon, mcon=mcon, tcon=tcon,
                                                train_dataloader=na_train_loader_1, validation_dataloader=na_val_loader_1
                                                )
            helper_train_MP.test(dataloader=na_val_loader_1, model=model_mp, encoder=model_autoencoder.encoder, loss_fn=loss_fn, tcon=tcon, dcon=dcon, mcon=mcon)
            
            ## Define Synthetic Denoising Autoencoder
            model_mp.eval()
            model_autoencoder.eval()
            dcon2 = dcon.copy()
            dcon2['replacement'] = 0
            dcon2['regenerate'] = True

            mcon2 = {
                'architecture': 'synthetic_dae',
                'loss': 'full', # full or focused
                'epochs': sdae_epoch,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 2000, 2000],
                'layer_dims_dec': [2000, 2000, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': str(model_mp),
                'encoder': str(model_autoencoder),
                'corruption_share': -1, # the share of features that are corrupted in the training of the DAE, -1 means that all missingness generated by the MP model is used
                'mask_between_epochs': 'equal', # equal or random, determines the scope of the random generator that is passed to the mask bernoulli sampling function
                'additional_noise': 0, # the share of additional noise that is added to the data during training

            }
            if 'MNIST' in dcon2['dataset']:
                mcon2['layer_dims_enc'][0] = 784
                mcon2['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon2['dataset']:
                mcon2['layer_dims_enc'][0] = 1024
                mcon2['layer_dims_dec'][-1] = 1024


            tcon2 = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 10, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }

            torch.random.manual_seed(1+cv_iteration)
            data_without_nas_2 = data.ImputationDatasetGen(config=dcon2, missing_vals=False)
            data_with_nas_2 = data.ImputationDatasetGen(config=dcon2, missing_vals=True)

            nona_train_loader_2 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'train', tcon2['train_val_test_split']), batch_size=mcon2['batch_size'], shuffle=True)
            nona_val_loader_2 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'validation', tcon2['train_val_test_split']), batch_size=mcon2['batch_size'], shuffle=False)
            nona_test_loader_2 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'test', tcon2['train_val_test_split']), batch_size=mcon2['batch_size'], shuffle=False)

            na_test_loader_2 = DataLoader(data.DatasetWithSplits(data_with_nas_2, 'test', [0, 0, 1]), batch_size=mcon2['batch_size'], shuffle=False) #here shuffle false, because it is only used for testing
            model_sdae = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=model_mp, layer_dims_enc=mcon2['layer_dims_enc'], layer_dims_dec=mcon2['layer_dims_dec'], relu=mcon2['relu'], image=mcon2['image']).to(device)
            loss_fn_sdae = nn.MSELoss(reduction='none')
            optimizer_sdae = torch.optim.Adam(model_sdae.parameters(), lr=mcon2['learning_rate'])
            scheduler_sdae = StepLR(optimizer_sdae, step_size=mcon2['step_size'], gamma=mcon2['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_sdae, encoder=model_autoencoder.encoder, loss_fn=loss_fn_sdae, optimizer=optimizer_sdae, scheduler=scheduler_sdae,
                                                dcon=dcon2, mcon=mcon2, tcon=tcon2,
                                                train_dataloader=nona_train_loader_2, validation_dataloader=nona_val_loader_2, test_dataloader=na_test_loader_2,
                                                noise_model=model_mp)
            mse_all[0, iteration, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_2, model=model_sdae, loss_fn=loss_fn_sdae, dcon=dcon2, mcon=mcon2, tcon=tcon2)

            ### Downstream Task
            model_sdae.eval()
            imputed_data = model_sdae(data_with_nas_2.data.to(device))
            imputed_targets = data_with_nas_2.labels

            full_data = torch.cat((data_without_nas_2.data.cpu(), imputed_data.cpu()), dim=0).detach()
            full_targets = torch.cat((data_without_nas_2.labels.cpu(), imputed_targets.cpu()), dim=0).detach()

            accuracy_all[0, iteration, cv_iteration-start_seed] = utils.softmaxRegression(full_data, full_targets, num_classes=10, num_epochs=10)

            ## Benchmark Models
            ### Benchmark DAE

            dcon3 = dcon2.copy()

            mcon3 = {
                'architecture': 'benchmark_dae',
                'loss': 'full', #full or focused
                'epochs': bdae_epoch,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 2000],
                'layer_dims_dec': [2000, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': dae_noise,
                'corruption_share': noise_level, # the share of features that are corrupted in the training of the DAE, here is set to actual noise level since that is also known in practice
                'mask_between_epochs': 'equal', # (DOES NOT APPLY to benchmark_DAE)
                'additional_noise': 0, # the share of additional noise that is added to the data during training
            }
            if 'MNIST' in dcon3['dataset']:
                mcon3['layer_dims_enc'][0] = 784
                mcon3['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon3['dataset']:
                mcon3['layer_dims_enc'][0] = 1024
                mcon3['layer_dims_dec'][-1] = 1024

            tcon3 = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 10, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }
            torch.random.manual_seed(1+cv_iteration)

            nona_train_loader_3 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'train', tcon3['train_val_test_split']), batch_size=mcon3['batch_size'], shuffle=True)
            nona_val_loader_3 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'validation', tcon3['train_val_test_split']), batch_size=mcon3['batch_size'], shuffle=False)
            nona_test_loader_3 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'test', tcon3['train_val_test_split']), batch_size=mcon3['batch_size'], shuffle=False)

            na_test_loader_3 = DataLoader(data.DatasetWithSplits(data_with_nas_2, 'test', [0, 0, 1]), batch_size=mcon3['batch_size'], shuffle=False) #here shuffle false, because it is only used for testing
            model_bdae = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=dae_noise, layer_dims_enc=mcon3['layer_dims_enc'], layer_dims_dec=mcon3['layer_dims_dec'], relu=mcon3['relu'], image=mcon3['image']).to(device)
            loss_fn_bdae = nn.MSELoss(reduction='none')
            optimizer_bdae = torch.optim.Adam(model_bdae.parameters(), lr=mcon3['learning_rate'])
            scheduler_bdae = StepLR(optimizer_bdae, step_size=mcon3['step_size'], gamma=mcon3['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_bdae, encoder=None, loss_fn=loss_fn_bdae, optimizer=optimizer_bdae, scheduler=scheduler_bdae,
                                                dcon=dcon3, mcon=mcon3, tcon=tcon3,
                                                train_dataloader=nona_train_loader_3, validation_dataloader=nona_val_loader_3, test_dataloader=na_test_loader_3,
                                                noise_model=dae_noise)
            mse_all[1, iteration, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_3, model=model_bdae, loss_fn=loss_fn_bdae, dcon=dcon3, mcon=mcon3, tcon=tcon3)

            model_bdae.eval()
            imputed_data = model_bdae(data_with_nas_2.data.to(device).detach())
            imputed_targets = data_with_nas_2.labels

            full_data = torch.cat((data_without_nas_2.data.cpu(), imputed_data.cpu()), dim=0).detach()
            full_targets = torch.cat((data_without_nas_2.labels.cpu(), imputed_targets.cpu()), dim=0).detach()

            accuracy_all[1, iteration, cv_iteration-start_seed] = utils.softmaxRegression(full_data, full_targets, num_classes=10, num_epochs=10)
            ### Benchmark VAE
            mcon4 = {
                'architecture': 'benchmark_vae',
                'loss': 'full', # full or focused
                'epochs': bvae_epoch,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 500],
                'layer_dims_dec': [500, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': None,
                'corruption_share': 0.0, # (DOES NOT APPLY to benchmark_VAE)
                'mask_between_epochs': 'equal', # (DOES NOT APPLY to benchmark_VAE)
                'additional_noise': 0, # the share of additional noise that is added to the data during training
            }
            if 'MNIST' in dcon3['dataset']:
                mcon4['layer_dims_enc'][0] = 784
                mcon4['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon3['dataset']:
                mcon4['layer_dims_enc'][0] = 1024
                mcon4['layer_dims_dec'][-1] = 1024

            tcon4 = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 10, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }
            model_bvae = modelSDAE.ImputeVAE(layer_dims_enc=mcon4['layer_dims_enc'], layer_dims_dec=mcon4['layer_dims_dec'], relu=mcon4['relu'], image=mcon4['image']).to(device)
            loss_fn_bvae = nn.MSELoss(reduction='none')
            optimizer_bvae = torch.optim.Adam(model_bvae.parameters(), lr=mcon4['learning_rate'])
            scheduler_bvae = StepLR(optimizer_bvae, step_size=mcon4['step_size'], gamma=mcon4['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_bvae, encoder=None, loss_fn=loss_fn_bvae, optimizer=optimizer_bvae, scheduler=scheduler_bvae,
                                                dcon=dcon3, mcon=mcon4, tcon=tcon4,
                                                train_dataloader=nona_train_loader_3, validation_dataloader=nona_val_loader_3, test_dataloader=na_test_loader_3,
                                                noise_model=None)
            mse_all[2, iteration, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_3, model=model_bvae, loss_fn=loss_fn_bvae, dcon=dcon3, mcon=mcon4, tcon=tcon4)

            model_bvae.eval()
            imputed_data = model_bvae(data_with_nas_2.data.to(device).detach())
            imputed_targets = data_with_nas_2.labels

            full_data = torch.cat((data_without_nas_2.data.cpu(), imputed_data.cpu()), dim=0).detach()
            full_targets = torch.cat((data_without_nas_2.labels.cpu(), imputed_targets.cpu()), dim=0).detach()

            accuracy_all[2, iteration, cv_iteration-start_seed] = utils.softmaxRegression(full_data, full_targets, num_classes=10, num_epochs=10)

            # Mean imputation
            data_with_nas_as_nas = data_with_nas_2.data
            mask = data_with_nas_2.targets
            data_with_nas_as_nas[mask == 1] = float('nan')
            full_data = torch.cat((data_without_nas_2.data.cpu(), data_with_nas_as_nas.cpu()), dim=0).detach()
            full_targets = torch.cat((data_without_nas_2.labels.cpu(), data_with_nas_2.labels.cpu()), dim=0).detach()

            full_data_imputed = torch.tensor(SimpleImputer().fit_transform(full_data)).float().to(device)
            ground_truth = data_with_nas_2.unmissing_data
            imputed_data = full_data_imputed[range(data_with_nas_as_nas.size(0)), :]

            mse = torch.sum(nn.MSELoss(reduction='none')(imputed_data.cpu(), ground_truth.cpu()) * (mask.cpu() == 1).float()) / torch.sum(mask.cpu())
            print(f'MSE: {mse}, RMSE: {torch.sqrt(mse)}')
            mse_all[3, iteration, cv_iteration-start_seed] = mse

            accuracy_all[3, iteration, cv_iteration-start_seed] = utils.softmaxRegression(full_data_imputed, full_targets, num_classes=10, num_epochs=10)

    # save results to csv
    mse_this_run = torch.transpose(torch.transpose(mse_all, 0, 2), 1, 2)[cv_iteration-start_seed, :, :].cpu().numpy()
    accuracy_this_run = torch.transpose(torch.transpose(accuracy_all, 0, 2), 1, 2)[cv_iteration-start_seed, :, :].cpu().numpy()

    mse_this_run_df = pd.DataFrame(mse_this_run, 
                                   columns=['Patch_10', 'Patch_20', 'Patches_10', 'Patches_20', 
                                            'MAR_10', 'MAR_20', 'MNAR_10', 'MNAR_20', 'MCAR_10', 'MCAR_20', 'QMNAR_10', 'QMNAR_20'],
                                   index=['SDAE', 'DAE', 'VAE', 'Mean'])
    accuracy_this_run_df = pd.DataFrame(accuracy_this_run, 
                                        columns=['Patch_10', 'Patch_20', 'Patches_10', 'Patches_20', 
                                                 'MAR_10', 'MAR_20', 'MNAR_10', 'MNAR_20', 'MCAR_10', 'MCAR_20', 'QMNAR_10', 'QMNAR_20'],
                                                 index=['SDAE', 'DAE', 'VAE', 'Mean'])

    mse_this_run_df.to_csv(f'results/full_model_{dataset}/mse_all_{cv_iteration}.csv')
    accuracy_this_run_df.to_csv(f'results/full_model_{dataset}/accuracy_{cv_iteration}.csv')

## create Excel file with rmse, accuracy and their standard deviations
# import all the csv files from the results folder
results_mse = []
results_accuracy = []
for file in os.listdir(folder_path):
    if file.endswith('.csv'):
        if 'mse_all' in file:
            results_mse.append(pd.read_csv(os.path.join(folder_path, file), header=0, index_col=0))
        elif 'accuracy' in file:
            results_accuracy.append(pd.read_csv(os.path.join(folder_path, file), header=0, index_col=0))
        else:
            raise ValueError('File name not supported')
        
results_mse = torch.tensor(np.array([df.values for df in results_mse]))
results_accuracy = torch.tensor(np.array([df.values for df in results_accuracy]))
mean_rmse = torch.mean(torch.sqrt(results_mse), dim=0)
std_rmse = torch.std(torch.sqrt(results_mse), dim=0)
mean_accuracy = torch.mean(results_accuracy, dim=0)
std_accuracy = torch.std(results_accuracy, dim=0)

# save as xlsx files
with pd.ExcelWriter(f'{folder_path}/mean_results.xlsx') as writer:
    pd.DataFrame(mean_rmse).to_excel(writer, sheet_name='mean_rmse')
    pd.DataFrame(std_rmse).to_excel(writer, sheet_name='std_rmse')
    pd.DataFrame(mean_accuracy).to_excel(writer, sheet_name='mean_accuracy')
    pd.DataFrame(std_accuracy).to_excel(writer, sheet_name='std_accuracy')