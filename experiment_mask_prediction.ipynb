{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import importlib\n",
    "import utils\n",
    "import data\n",
    "import modelMP\n",
    "import modelSDAE\n",
    "import helper_train_MP\n",
    "import helper_train_SDAE\n",
    "import helper_train_MP_GAN\n",
    "import helper_noise\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "importlib.reload(modelMP)\n",
    "importlib.reload(helper_train_MP)\n",
    "importlib.reload(modelSDAE)\n",
    "importlib.reload(helper_train_SDAE)\n",
    "importlib.reload(helper_noise)\n",
    "importlib.reload(helper_train_MP_GAN)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "torch.random.manual_seed(1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# randomness for the training of the Benchmark DAE\n",
    "def dae_noise(x):\n",
    "    return torch.rand_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dcon = {\n",
    "    'dataset': 'FashionMNIST', # one of 'MNIST', 'FashionMNIST', 'CIFAR10'\n",
    "    'noise_mechanism': 'mcar', # (Does not apply here. The dataset for the MP model is created inside the for loop)\n",
    "    'na_obs_percentage': 0.4, # the number of observations that have missing values\n",
    "    'replacement': 'uniform', # what value is plugged in for missing values in the observations with missing values (number or 'uniform')\n",
    "    'noise_level': 0.3, # the percentage of missing values per observation (share of features that are missing)\n",
    "    'download': False,\n",
    "    'regenerate': True,\n",
    "    'device': device,\n",
    "\n",
    "\n",
    "    # for noise_mechanism 'patches'\n",
    "    'patch_size_axis': 5, # size of the patch in the x and y direction\n",
    "\n",
    "    # for noise_mechanism MAR and MNAR\n",
    "    'randperm_cols': False, # whether to shuffle the columns of the data matrix before applying the noise mechanism\n",
    "    'average_missing_rates': 'normal', # can be 'uniform', 'normal' or a list/tuple/tensor of length no. of features, determines how the missingness rates are generated\n",
    "    # if uniform, noise_level is used as mean for uniform distribution and impossible values are clipped to the interval [0,1] --> noise_level != exact missingness rate in mask\n",
    "    # if normal, noise_level is ignored and the options below apply\n",
    "    'chol_eps': 1e-6, # epsilon for the cholesky decomposition (epsilon*I is added to the covariance matrix to make it positive definite)\n",
    "    'sigmoid_offset': 0.15, # offset for the sigmoid function applied to the average missing rates generated by MultivariateNormal\n",
    "    'sigmoid_k': 10, # steepness of sigmoid\n",
    "\n",
    "    # for MNAR only\n",
    "    'dependence': 'simple_unobserved', # can be 'simple_unobserved', 'complex_unobserved', 'unobserved_and_observed'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "data_without_nas_1 = data.ImputationDatasetGen(config=dcon, missing_vals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcon0 = {\n",
    "    'architecture': 'encoder_model_dae',\n",
    "    'loss': 'full', # must be full otherwise error\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 3e-4,\n",
    "    'lr_decay': False,\n",
    "    'gamma': 2e-4,\n",
    "    'step_size': 45,\n",
    "    'layer_dims_enc': [784, 2000, 50],\n",
    "    'layer_dims_dec': [50, 2000, 784],\n",
    "    'device': device,\n",
    "    'relu': True,\n",
    "    'image': True,\n",
    "    'noise_model': dae_noise,\n",
    "    'corruption_share': 0.2, #blevel of the masking noise that is used for training the DAE\n",
    "    'mask_between_epochs': 'equal', # equal or random, determines the scope of the random generator that is passed to the mask bernoulli sampling function\n",
    "    'additional_noise': 0, # does not apply here\n",
    "}\n",
    "\n",
    "tcon0 = {\n",
    "    'new_training': 1,\n",
    "    'log': 0,\n",
    "    'save_model': 0,\n",
    "    'img_index': 4, # index of the image to be plotted\n",
    "    'activations': 1,\n",
    "    'device': device,\n",
    "    'train_val_test_split': [0.8, 0.2, 0]\n",
    "}\n",
    "\n",
    "if 'MNIST' in dcon['dataset']:\n",
    "    mcon0['layer_dims_enc'][0] = 784\n",
    "    mcon0['layer_dims_dec'][-1] = 784\n",
    "elif 'CIFAR10' in dcon['dataset']:\n",
    "    mcon0['layer_dims_enc'][0] = 1024\n",
    "    mcon0['layer_dims_dec'][-1] = 1024\n",
    "    \n",
    "nona_train_loader_0 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'train', tcon0['train_val_test_split']), batch_size=mcon0['batch_size'], shuffle=True)\n",
    "nona_val_loader_0 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'validation', tcon0['train_val_test_split']), batch_size=mcon0['batch_size'], shuffle=False)\n",
    "\n",
    "model_autoencoder = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=dae_noise, layer_dims_enc=mcon0['layer_dims_enc'], layer_dims_dec=mcon0['layer_dims_dec'], relu=mcon0['relu'], image=mcon0['image']).to(device)\n",
    "loss_fn_autoencoder = nn.MSELoss(reduction='none')\n",
    "optimizer_autoencoder = torch.optim.Adam(model_autoencoder.parameters(), lr=mcon0['learning_rate'])\n",
    "scheduler_autoencoder = StepLR(optimizer_autoencoder, step_size=mcon0['step_size'], gamma=mcon0['gamma'])\n",
    "print(model_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_train_SDAE.train_imputation_model(model=model_autoencoder, encoder=None, loss_fn=loss_fn_autoencoder, optimizer=optimizer_autoencoder, scheduler=scheduler_autoencoder,\n",
    "                                    dcon=dcon, mcon=mcon0, tcon=tcon0,\n",
    "                                    train_dataloader=nona_train_loader_0, validation_dataloader=nona_val_loader_0,\n",
    "                                    noise_model=dae_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_all = []\n",
    "rmse_all_no_enc = []\n",
    "accuracy_all = []\n",
    "accuracy_all_no_enc = []\n",
    "corr_all = []\n",
    "corr_all_no_enc = []\n",
    "images = torch.zeros(3, 6, 784) if 'MNIST' in dcon['dataset'] else torch.zeros(3, 6, 1024)\n",
    "idx = 10\n",
    "for i, noise_mechanism in enumerate([helper_noise.missingness_adder_patch, helper_noise.missingness_adder_mnar, helper_noise.missingness_adder_mcar]):\n",
    "    dcon_temp = {\n",
    "        'dataset': dcon['dataset'],\n",
    "        'noise_mechanism': ['patch', 'mnar', 'mcar'][i], # missingness mechanism\n",
    "        'na_obs_percentage': 0.4, # the number of observations that have missing values\n",
    "        'replacement': 'uniform', # what value is plugged in for missing values in the observations with missing values (number or 'uniform')\n",
    "        'noise_level': 0.2, # the percentage of missing values per observation (share of features that are missing)\n",
    "        'download': False,\n",
    "        'regenerate': True,\n",
    "        'device': device,\n",
    "\n",
    "\n",
    "        # for noise_mechanism 'patches'\n",
    "        'patch_size_axis': 5, #size of the patch in the x and y direction\n",
    "\n",
    "        # for noise_mechanism MAR and MNAR\n",
    "        'randperm_cols': False, #whether to shuffle the columns of the data matrix before applying the noise mechanism\n",
    "        'average_missing_rates': 'normal', #can be 'uniform', 'normal' or a list/tuple/tensor of length no. of features, determines how the missingness rates are generated\n",
    "        # if uniform, noise_level is used as mean for uniform distribution and impossible values are clipped to the interval [0,1] --> noise_level != exact missingness rate in mask\n",
    "        # if normal, noise_level is ignored and the options below apply\n",
    "        'chol_eps': 1e-6, #epsilon for the cholesky decomposition (epsilon*I is added to the covariance matrix to make it positive definite)\n",
    "        'sigmoid_offset': 0.15, #offset for the sigmoid function applied to the average missing rates generated by MultivariateNormal\n",
    "        'sigmoid_k': 10, #steepness of sigmoid\n",
    "\n",
    "        # for MNAR only\n",
    "        'dependence': 'simple_unobserved', #can be 'simple_unobserved', 'complex_unobserved', 'unobserved_and_observed'\n",
    "\n",
    "    }\n",
    "\n",
    "    mcon = {\n",
    "        'architecture': 'mask_pred_mlp',\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 3e-4,\n",
    "        'lr_decay': False,\n",
    "        'gamma': 2e-4,\n",
    "        'step_size': 45,\n",
    "        'layer_dims': [784, 2000, 2000, 2000, 784],\n",
    "        'dropout': 0.5,\n",
    "        'device': device,\n",
    "        'relu': True,\n",
    "        'image': True,\n",
    "        'encoder': str(model_autoencoder),    \n",
    "    }\n",
    "    mcon_no_enc = {\n",
    "        'architecture': 'mask_pred_mlp',\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 3e-4,\n",
    "        'lr_decay': False,\n",
    "        'gamma': 2e-4,\n",
    "        'step_size': 45,\n",
    "        'layer_dims': [784, 2000, 2000, 2000, 784],\n",
    "        'dropout': 0.5,\n",
    "        'device': device,\n",
    "        'relu': True,\n",
    "        'image': True,\n",
    "        'encoder': str(model_autoencoder),    \n",
    "    }\n",
    "\n",
    "    tcon = {\n",
    "        'new_training': 1,\n",
    "        'log': 0,\n",
    "        'save_model': 0,\n",
    "        'img_index': 4, # index of the image to be plotted\n",
    "        'activations': 0,\n",
    "        'device': device,\n",
    "        'train_val_test_split': [0.8, 0.2, 0]\n",
    "    }\n",
    "\n",
    "    if 'MNIST' in dcon['dataset']:\n",
    "        mcon['layer_dims'][0] = mcon0['layer_dims_enc'][-1]\n",
    "        mcon['layer_dims'][-1] = 784\n",
    "        mcon_no_enc['layer_dims'][0] = 784\n",
    "        mcon_no_enc['layer_dims'][-1] = 784\n",
    "\n",
    "    elif 'CIFAR10' in dcon['dataset']:\n",
    "        mcon['layer_dims'][0] = mcon0['layer_dims_enc'][-1]\n",
    "        mcon['layer_dims'][-1] = 1024\n",
    "        mcon_no_enc['layer_dims'][0] = 1024\n",
    "        mcon_no_enc['layer_dims'][-1] = 1024\n",
    "\n",
    "    torch.random.manual_seed(1)\n",
    "    data_with_nas_1 = data.ImputationDatasetGen(config=dcon_temp, missing_vals=True)\n",
    "\n",
    "    na_train_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'train', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=True)\n",
    "    na_val_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'validation', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)\n",
    "    na_test_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'test', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)\n",
    "\n",
    "    model_mp = modelMP.MaskPredMLP(layer_dims=mcon['layer_dims'], dropout=mcon['dropout'], relu=mcon['relu'], image=mcon['image']).to(device)\n",
    "    model_mp_no_enc = modelMP.MaskPredMLP(layer_dims=mcon_no_enc['layer_dims'], dropout=mcon_no_enc['dropout'], relu=mcon_no_enc['relu'], image=mcon_no_enc['image']).to(device)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    loss_fn_no_enc = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model_mp.parameters(), lr=mcon['learning_rate'])\n",
    "    optimizer_no_enc = torch.optim.Adam(model_mp_no_enc.parameters(), lr=mcon_no_enc['learning_rate'])\n",
    "    scheduler = StepLR(optimizer, step_size=mcon['step_size'], gamma=mcon['gamma'])\n",
    "\n",
    "    helper_train_MP.train_model(model=model_mp, encoder=model_autoencoder.encoder, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler,\n",
    "                                        dcon=dcon_temp, mcon=mcon, tcon=tcon,\n",
    "                                        train_dataloader=na_train_loader_1, validation_dataloader=na_val_loader_1\n",
    "                                        )\n",
    "    helper_train_MP.train_model(model=model_mp_no_enc, encoder=None, loss_fn=loss_fn_no_enc, optimizer=optimizer_no_enc, scheduler=scheduler,\n",
    "                                        dcon=dcon_temp, mcon=mcon_no_enc, tcon=tcon,\n",
    "                                        train_dataloader=na_train_loader_1, validation_dataloader=na_val_loader_1\n",
    "                                        )\n",
    "    ## Comparison Learned and True Masks\n",
    "    _, mask_true = noise_mechanism(dataset=data_without_nas_1.data, config=dcon)\n",
    "\n",
    "    model_autoencoder.eval()\n",
    "    model_mp.eval()\n",
    "    model_mp_no_enc.eval()\n",
    "    mask_probs = model_mp(model_autoencoder.encoder(data_without_nas_1.data.to(device)))\n",
    "    mask_probs_no_enc = model_mp_no_enc(data_without_nas_1.data.to(device))\n",
    "    mask_gen = torch.bernoulli(mask_probs).detach()\n",
    "    mask_gen_no_enc = torch.bernoulli(mask_probs_no_enc).detach()\n",
    "\n",
    "\n",
    "    corrupted_true= mask_true[None, idx].cpu()\n",
    "    corrupted_gen = mask_gen[None, idx].cpu()\n",
    "    corrupted_gen_no_enc = mask_gen_no_enc[None, idx].cpu()\n",
    "    mean_mask_true = torch.mean(mask_true, dim=0, keepdim=True).cpu()\n",
    "    mean_mask_gen = torch.mean(mask_gen, dim=0, keepdim=True).cpu()\n",
    "    mean_mask_gen_no_enc = torch.mean(mask_gen_no_enc, dim=0, keepdim=True).cpu()\n",
    "\n",
    "    images[i] = torch.cat([corrupted_true, corrupted_gen, corrupted_gen_no_enc, \n",
    "                           mean_mask_true, mean_mask_gen, mean_mask_gen_no_enc], dim=0)\n",
    "\n",
    "    # compute rmse between true and generated missingness shares\n",
    "    rmse = torch.sqrt(torch.mean((mean_mask_true - mean_mask_gen)**2))\n",
    "    rmse_no_enc = torch.sqrt(torch.mean((mean_mask_true - mean_mask_gen_no_enc)**2))\n",
    "\n",
    "    # numpy correlation between the two sequences\n",
    "    corr_np = np.corrcoef(mean_mask_true.squeeze().numpy(), mean_mask_gen.squeeze().numpy())\n",
    "    corr_np_no_enc = np.corrcoef(mean_mask_true.squeeze().numpy(), mean_mask_gen_no_enc.squeeze().numpy())\n",
    "\n",
    "    accuracy = torch.mean((mask_true.cpu() == mask_gen.cpu()).float())\n",
    "    accuracy_no_enc = torch.mean((mask_true.cpu() == mask_gen_no_enc.cpu()).float())\n",
    "\n",
    "    rmse_all.append(rmse)\n",
    "    rmse_all_no_enc.append(rmse_no_enc)\n",
    "    corr_all.append(corr_np[0,1])\n",
    "    corr_all_no_enc.append(corr_np_no_enc[0,1])\n",
    "    accuracy_all.append(accuracy)\n",
    "    accuracy_all_no_enc.append(accuracy_no_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table with quantitative results\n",
    "df = pd.DataFrame([rmse_all, rmse_all_no_enc, corr_all, corr_all_no_enc, accuracy_all, accuracy_all_no_enc], columns=['patch', 'mnar', 'mcar'], index=['rmse', 'rmse_no_enc', 'corr', 'corr_no_enc', 'accuracy', 'accuracy_no_enc'])\n",
    "df = df.map(lambda x: x.item())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot with qualitative results\n",
    "# number of missingness mechanisms (rows)\n",
    "n_mech = 3\n",
    "fig, ax = plt.subplots(n_mech, 6, figsize=(16, 8))\n",
    "\n",
    "sizes = [28, 28]\n",
    "sizes = [32, 32] if 'CIFAR10' in dcon['dataset'] else sizes\n",
    "\n",
    "for i in range(n_mech):\n",
    "    for j in range(6):\n",
    "        ax[i, j].imshow(torch.unflatten(images[i], dim=1, sizes=sizes)[j].cpu().float(), cmap='gray')\n",
    "        ax[i, j].set_xticks([])\n",
    "        ax[i, j].set_yticks([])\n",
    "        ax[i, j].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "        for spine in ['top', 'right', 'bottom', 'left']:\n",
    "            ax[i, j].spines[spine].set_visible(False)\n",
    "\n",
    "x_labels = ['Simulated', 'MP', 'MPnoEnc', 'Avg Simulated', 'Avg MP', 'Avg MPnoEnc']\n",
    "y_labels = ['Patch', 'MNAR', 'MCAR']\n",
    "\n",
    "for j in range(6):\n",
    "    ax[0, j].set_xlabel(x_labels[j], labelpad=8, fontsize=12, va='center', ha='center')\n",
    "    ax[0, j].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "for j in range(6):\n",
    "    ax[1, j].set_xlabel(x_labels[j], labelpad=8, fontsize=12, va='center', ha='center')\n",
    "    ax[1, j].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "for i in range(3):\n",
    "    ax[i, 0].set_ylabel(y_labels[i], rotation=90, labelpad=8, fontsize=12, va='center', ha='center')\n",
    "    ax[i, 0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "plt.suptitle('Simulated and Learned Missingness Masks', fontsize=16, y=0.95, ha='center')\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
