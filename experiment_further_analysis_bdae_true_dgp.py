import os
os.environ["CUBLAS_WORKSPACE_CONFIG"]=":4096:8"

import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.utils.data
import pandas as pd
import numpy as np
from torch.optim.lr_scheduler import StepLR
import importlib
import utils
import data
import modelMP
import modelSDAE
import helper_train_MP
import helper_train_SDAE
import helper_train_MP_GAN
import helper_train_BDAE_true
import helper_noise

importlib.reload(utils)
importlib.reload(data)
importlib.reload(modelMP)
importlib.reload(helper_train_MP)
importlib.reload(modelSDAE)
importlib.reload(helper_train_SDAE)
importlib.reload(helper_noise)
importlib.reload(helper_train_MP_GAN)
importlib.reload(helper_train_BDAE_true)

if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True)
torch.random.manual_seed(1)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

# randomness for the training of the Benchmark DAE
def dae_noise(x):
    return torch.rand_like(x)

# some overall experiment parameters
EPOCHS = 10
start_seed = 0
n_seeds = 10
mse_all = torch.zeros(5, 3, n_seeds)
folder_path = 'results/further_analysis/bdae_true'

os.makedirs(folder_path, exist_ok=True)

for cv_iteration in range(start_seed, start_seed+n_seeds):
    for i, noise_mechanism in enumerate(['patch', 'mnar', 'mcar']):
        print(noise_mechanism)
        for j, replacement in enumerate(['uniform', 0]):
            torch.random.manual_seed(1+cv_iteration)
            dcon = {
                'dataset': 'FashionMNIST', # one of 'MNIST', 'FashionMNIST', 'CIFAR10'
                'noise_mechanism': noise_mechanism, # missingness mechanism
                'na_obs_percentage': 0.4, # the number of observations that have missing values
                'replacement': replacement, # what value is plugged in for missing values in the observations with missing values (number or 'uniform')
                'noise_level': 0.2, # the percentage of missing values per observation (share of features that are missing)
                'download': False,
                'regenerate': True,
                'device': device,

                # for noise_mechanism 'patches'
                'patch_size_axis': 5, # size of the patch in the x and y direction

                # for noise_mechanism MAR and MNAR
                'randperm_cols': False, # whether to shuffle the columns of the data matrix before applying the noise mechanism
                'average_missing_rates': 'normal', # can be 'uniform', 'normal' or a list/tuple/tensor of length no. of features, determines how the missingness rates are generated
                # if uniform, noise_level is used as mean for uniform distribution and impossible values are clipped to the interval [0,1] --> noise_level != exact missingness rate in mask
                # if normal, noise_level is ignored and the options below apply
                'chol_eps': 1e-6, # epsilon for the cholesky decomposition (epsilon*I is added to the covariance matrix to make it positive definite)
                'sigmoid_offset': 0.05, # offset for the sigmoid function applied to the average missing rates generated by MultivariateNormal
                'sigmoid_k': 10, # steepness of sigmoid

                # for MNAR only
                'dependence': 'simple_unobserved', # can be 'simple_unobserved', 'complex_unobserved', 'unobserved_and_observed'
            }
            
            torch.random.manual_seed(1+cv_iteration)
            data_without_nas_1 = data.ImputationDatasetGen(config=dcon, missing_vals=False)
            data_with_nas_1 = data.ImputationDatasetGen(config=dcon, missing_vals=True)

            ## Encoder model
            mcon0 = {
                'architecture': 'encoder_model_dae', # one of 'mask_pred_mlp', 'mask_pred_vae', 'mask_pred_gan', 'encoder_model_dae', 'encoder_model_vae'
                'loss': 'full', # must be full otherwise error
                'epochs': EPOCHS,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 700],
                'layer_dims_dec': [700, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': dae_noise,
                'corruption_share': 0.2, # level of the masking noise that is used for training the DAE
                'mask_between_epochs': 'equal', # equal or random, determines the scope of the random generator that is passed to the mask bernoulli sampling function
                'additional_noise': 0, # does not apply here
            }

            tcon0 = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 4, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }

            if 'MNIST' in dcon['dataset']:
                mcon0['layer_dims_enc'][0] = 784
                mcon0['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon['dataset']:
                mcon0['layer_dims_enc'][0] = 1024
                mcon0['layer_dims_dec'][-1] = 1024

                
            nona_train_loader_0 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'train', tcon0['train_val_test_split']), batch_size=mcon0['batch_size'], shuffle=True)
            nona_val_loader_0 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'validation', tcon0['train_val_test_split']), batch_size=mcon0['batch_size'], shuffle=False)

            model_autoencoder = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=dae_noise, layer_dims_enc=mcon0['layer_dims_enc'], layer_dims_dec=mcon0['layer_dims_dec'], relu=mcon0['relu'], image=mcon0['image']).to(device)
            loss_fn_autoencoder = nn.MSELoss(reduction='none')
            optimizer_autoencoder = torch.optim.Adam(model_autoencoder.parameters(), lr=mcon0['learning_rate'])
            scheduler_autoencoder = StepLR(optimizer_autoencoder, step_size=mcon0['step_size'], gamma=mcon0['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_autoencoder, encoder=None, loss_fn=loss_fn_autoencoder, optimizer=optimizer_autoencoder, scheduler=scheduler_autoencoder,
                                                dcon=dcon, mcon=mcon0, tcon=tcon0,
                                                train_dataloader=nona_train_loader_0, validation_dataloader=nona_val_loader_0,
                                                noise_model=dae_noise)
            ## MP model
            model_autoencoder.eval()
            mcon = {
                'architecture': 'mask_pred_mlp',
                'epochs': EPOCHS,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims': [784, 2000, 2000, 2000, 784],
                'dropout': 0.5,
                'device': device,
                'relu': True,
                'image': True,
                'encoder': str(model_autoencoder) 
            }

            tcon = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 4, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }

            if 'MNIST' in dcon['dataset']:
                mcon['layer_dims'][0] = mcon0['layer_dims_enc'][-1]
                mcon['layer_dims'][-1] = 784

            elif 'CIFAR10' in dcon['dataset']:
                mcon['layer_dims'][0] = mcon0['layer_dims_enc'][-1]
                mcon['layer_dims'][-1] = 1024



            nona_train_loader_1 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'train', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=True)
            nona_val_loader_1 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'validation', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)
            nona_test_loader_1 = DataLoader(data.DatasetWithSplits(data_without_nas_1, 'test', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)

            na_train_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'train', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=True)
            na_val_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'validation', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)
            na_test_loader_1 = DataLoader(data.DatasetWithSplits(data_with_nas_1, 'test', tcon['train_val_test_split']), batch_size=mcon['batch_size'], shuffle=False)

            model_mp = modelMP.MaskPredMLP(layer_dims=mcon['layer_dims'], dropout=mcon['dropout'], relu=mcon['relu'], image=mcon['image']).to(device)

            loss_fn = nn.BCELoss()
            optimizer = torch.optim.Adam(model_mp.parameters(), lr=mcon['learning_rate'])
            scheduler = StepLR(optimizer, step_size=mcon['step_size'], gamma=mcon['gamma'])

            # train model
            helper_train_MP.train_model(model=model_mp, encoder=model_autoencoder.encoder, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler,
                                                dcon=dcon, mcon=mcon, tcon=tcon,
                                                train_dataloader=na_train_loader_1, validation_dataloader=na_val_loader_1
                                                )        
            ## Define Synthetic Denoising Autoencoder
            model_mp.eval()
            model_autoencoder.eval()
            dcon2 = dcon.copy()
            dcon2['replacement'] = 0
            dcon2['regenerate'] = True

            mcon2 = {
                'architecture': 'synthetic_dae',
                'loss': 'full', # full or focused
                'epochs': EPOCHS,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 2000, 2000],
                'layer_dims_dec': [2000, 2000, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': str(model_mp),
                'encoder': str(model_autoencoder),
                'corruption_share': -1, # the share of features that are corrupted in the training of the DAE
                'mask_between_epochs': 'equal', # equal or random, determines the scope of the random generator that is passed to the mask bernoulli sampling function
                'additional_noise': 0, # the share of additional noise that is added to the data during training

            }
            if 'MNIST' in dcon['dataset']:
                mcon2['layer_dims_enc'][0] = 784
                mcon2['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon['dataset']:
                mcon2['layer_dims_enc'][0] = 1024
                mcon2['layer_dims_dec'][-1] = 1024


            tcon2 = {
                'new_training': 1,
                'log': 0,
                'save_model': 0,
                'img_index': 10, # index of the image to be plotted
                'activations': 0,
                'device': device,
                'train_val_test_split': [0.8, 0.2, 0]
            }

            torch.random.manual_seed(1+cv_iteration)
            data_without_nas_2 = data.ImputationDatasetGen(config=dcon2, missing_vals=False)
            data_with_nas_2 = data.ImputationDatasetGen(config=dcon2, missing_vals=True)

            nona_train_loader_2 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'train', tcon2['train_val_test_split']), batch_size=mcon2['batch_size'], shuffle=True)
            nona_val_loader_2 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'validation', tcon2['train_val_test_split']), batch_size=mcon2['batch_size'], shuffle=False)
            nona_test_loader_2 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'test', tcon2['train_val_test_split']), batch_size=mcon2['batch_size'], shuffle=False)

            na_test_loader_2 = DataLoader(data.DatasetWithSplits(data_with_nas_2, 'test', [0, 0, 1]), batch_size=mcon2['batch_size'], shuffle=False) # here shuffle false, because it is only used for testing
            model_sdae = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=model_mp, layer_dims_enc=mcon2['layer_dims_enc'], layer_dims_dec=mcon2['layer_dims_dec'], relu=mcon2['relu'], image=mcon2['image']).to(device)
            loss_fn_sdae = nn.MSELoss(reduction='none')
            optimizer_sdae = torch.optim.Adam(model_sdae.parameters(), lr=mcon2['learning_rate'])
            scheduler_sdae = StepLR(optimizer_sdae, step_size=mcon2['step_size'], gamma=mcon2['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_sdae, encoder=model_autoencoder.encoder, loss_fn=loss_fn_sdae, optimizer=optimizer_sdae, scheduler=scheduler_sdae,
                                                dcon=dcon2, mcon=mcon2, tcon=tcon2,
                                                train_dataloader=nona_train_loader_2, validation_dataloader=nona_val_loader_2, test_dataloader=na_test_loader_2,
                                                noise_model=model_mp)
            # skip the following models if replacement is not 'uniform'
            if replacement == 'uniform':
                mse_all[0, i, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_2, model=model_sdae, loss_fn=loss_fn_sdae, dcon=dcon, mcon=mcon2, tcon=tcon2)
            else:
                mse_all[2, i, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_2, model=model_sdae, loss_fn=loss_fn_sdae, dcon=dcon, mcon=mcon2, tcon=tcon2)
                continue 

            # MP+SDAE model without encoder (only for 'uniform' replacement)
            mcon3 = {
                'architecture': 'mask_pred_mlp_no_enc',
                'epochs': EPOCHS,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims': [784, 2000, 2000, 2000, 784],
                'dropout': 0.5,
                'device': device,
                'relu': True,
                'image': True,
                'encoder': None 
            }
            if 'MNIST' in dcon['dataset']:
                mcon3['layer_dims'][0] = 784
                mcon3['layer_dims'][-1] = 784

            elif 'CIFAR10' in dcon['dataset']:
                mcon3['layer_dims'][0] = 1024
                mcon3['layer_dims'][-1] = 1024


            model_mp_no_enc = modelMP.MaskPredMLP(layer_dims=mcon3['layer_dims'], dropout=mcon3['dropout'], relu=mcon3['relu'], image=mcon3['image']).to(device)
            loss_fn_no_enc = nn.BCELoss()
            optimizer_no_enc = torch.optim.Adam(model_mp_no_enc.parameters(), lr=mcon3['learning_rate'])
            scheduler_no_enc = StepLR(optimizer_no_enc, step_size=mcon3['step_size'], gamma=mcon3['gamma'])

            # train model
            helper_train_MP.train_model(model=model_mp_no_enc, encoder=None, loss_fn=loss_fn_no_enc, optimizer=optimizer_no_enc, scheduler=scheduler_no_enc,
                                                dcon=dcon, mcon=mcon3, tcon=tcon,
                                                train_dataloader=na_train_loader_1, validation_dataloader=na_val_loader_1
                                                )
                    
            ## Define Synthetic Denoising Autoencoder without Encoder
            model_mp_no_enc.eval()

            mcon4 = {
                'architecture': 'synthetic_dae_no_enc',
                'loss': 'full', # full or focused
                'epochs': EPOCHS,
                'batch_size': 64,
                'learning_rate': 3e-4,
                'lr_decay': False,
                'gamma': 2e-4,
                'step_size': 45,
                'layer_dims_enc': [784, 2000, 2000, 2000],
                'layer_dims_dec': [2000, 2000, 2000, 784],
                'device': device,
                'relu': True,
                'image': True,
                'noise_model': str(model_mp_no_enc),
                'encoder': None,
                'corruption_share': -1, # the share of features that are corrupted in the training of the DAE
                'mask_between_epochs': 'equal', # equal or random, determines the scope of the random generator that is passed to the mask bernoulli sampling function
                'additional_noise': 0, # the share of additional noise that is added to the data during training

            }
            if 'MNIST' in dcon['dataset']:
                mcon4['layer_dims_enc'][0] = 784
                mcon4['layer_dims_dec'][-1] = 784
            elif 'CIFAR10' in dcon['dataset']:
                mcon4['layer_dims_enc'][0] = 1024
                mcon4['layer_dims_dec'][-1] = 1024


            torch.random.manual_seed(1+cv_iteration)

            model_sdae_no_enc = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=model_mp_no_enc, layer_dims_enc=mcon4['layer_dims_enc'], layer_dims_dec=mcon4['layer_dims_dec'], relu=mcon4['relu'], image=mcon4['image']).to(device)
            loss_fn_sdae_no_enc = nn.MSELoss(reduction='none')
            optimizer_sdae_no_enc = torch.optim.Adam(model_sdae_no_enc.parameters(), lr=mcon4['learning_rate'])
            scheduler_sdae_no_enc = StepLR(optimizer_sdae_no_enc, step_size=mcon4['step_size'], gamma=mcon4['gamma'])
            helper_train_SDAE.train_imputation_model(model=model_sdae_no_enc, encoder=None, loss_fn=loss_fn_sdae_no_enc, optimizer=optimizer_sdae_no_enc, scheduler=scheduler_sdae_no_enc,
                                                dcon=dcon, mcon=mcon4, tcon=tcon2,
                                                train_dataloader=nona_train_loader_2, validation_dataloader=nona_val_loader_2, test_dataloader=na_test_loader_2,
                                                noise_model=model_mp_no_enc)
            mse_all[1, i, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_2, model=model_sdae_no_enc, loss_fn=loss_fn_sdae_no_enc, dcon=dcon, mcon=mcon4, tcon=tcon2)



        ## Benchmark Models ####################################################################################
        ### Benchmark DAE
        mcon5 = {
            'architecture': 'benchmark_dae',
            'loss': 'full', # full or focused
            'epochs': EPOCHS,
            'batch_size': 64,
            'learning_rate': 3e-4,
            'lr_decay': False,
            'gamma': 2e-4,
            'step_size': 45,
            'layer_dims_enc': [784, 2000, 2000],
            'layer_dims_dec': [2000, 2000, 784],
            'device': device,
            'relu': True,
            'image': True,
            'noise_model': dae_noise,
            'corruption_share': 0.2, # the share of features that are corrupted in the training of the DAE
            'mask_between_epochs': 'random', #(DOES NOT APPLY to benchmark_DAE)
            'additional_noise': 0, #vthe share of additional noise that is added to the data during training
        }
        if 'MNIST' in dcon['dataset']:
            mcon5['layer_dims_enc'][0] = 784
            mcon5['layer_dims_dec'][-1] = 784
        elif 'CIFAR10' in dcon['dataset']:
            mcon5['layer_dims_enc'][0] = 1024
            mcon5['layer_dims_dec'][-1] = 1024

        tcon3 = {
            'new_training': 1,
            'log': 0,
            'save_model': 0,
            'img_index': 10, # index of the image to be plotted
            'activations': 0,
            'device': device,
            'train_val_test_split': [0.8, 0.2, 0]
        }
        torch.random.manual_seed(1+cv_iteration)

        nona_train_loader_3 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'train', tcon3['train_val_test_split']), batch_size=mcon5['batch_size'], shuffle=True)
        nona_val_loader_3 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'validation', tcon3['train_val_test_split']), batch_size=mcon5['batch_size'], shuffle=False)
        nona_test_loader_3 = DataLoader(data.DatasetWithSplits(data_without_nas_2, 'test', tcon3['train_val_test_split']), batch_size=mcon5['batch_size'], shuffle=False)

        na_test_loader_3 = DataLoader(data.DatasetWithSplits(data_with_nas_2, 'test', [0, 0, 1]), batch_size=mcon5['batch_size'], shuffle=False) #here shuffle false, because it is only used for testing
        model_bdae = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=dae_noise, layer_dims_enc=mcon5['layer_dims_enc'], layer_dims_dec=mcon5['layer_dims_dec'], relu=mcon5['relu'], image=mcon5['image']).to(device)
        loss_fn_bdae = nn.MSELoss(reduction='none')
        optimizer_bdae = torch.optim.Adam(model_bdae.parameters(), lr=mcon5['learning_rate'])
        scheduler_bdae = StepLR(optimizer_bdae, step_size=mcon5['step_size'], gamma=mcon5['gamma'])
        helper_train_SDAE.train_imputation_model(model=model_bdae, encoder=None, loss_fn=loss_fn_bdae, optimizer=optimizer_bdae, scheduler=scheduler_bdae,
                                            dcon=dcon, mcon=mcon5, tcon=tcon3,
                                            train_dataloader=nona_train_loader_3, validation_dataloader=nona_val_loader_3, test_dataloader=na_test_loader_3,
                                            noise_model=dae_noise)
        mse_all[3, i, cv_iteration-start_seed] = helper_train_SDAE.test(dataloader=na_test_loader_3, model=model_bdae, loss_fn=loss_fn_bdae, dcon=dcon, mcon=mcon5, tcon=tcon3)
        
        ### Benchmark DAE Trained with True Missingness Mechanism
        mcon6 = {
            'architecture': 'benchmark_dae_true_dgp',
            'loss': 'full', # full or focused
            'epochs': EPOCHS,
            'batch_size': 64,
            'learning_rate': 3e-4,
            'lr_decay': False,
            'gamma': 2e-4,
            'step_size': 45,
            'layer_dims_enc': [784, 2000, 2000],
            'layer_dims_dec': [2000, 2000, 784],
            'device': device,
            'relu': True,
            'image': True,
            'noise_model': 'None, trained with true missingness mechanism (already in dataset)',
            'corruption_share': -1, # the share of features that are corrupted in the training of the DAE
            'mask_between_epochs': 'random', # (DOES NOT APPLY to benchmark_DAE)
            'additional_noise': 0, # does not apply to benchmark_DAE
        }
        if 'MNIST' in dcon['dataset']:
            mcon6['layer_dims_enc'][0] = 784
            mcon6['layer_dims_dec'][-1] = 784
        elif 'CIFAR10' in dcon['dataset']:
            mcon6['layer_dims_enc'][0] = 1024
            mcon6['layer_dims_dec'][-1] = 1024

        temp_dcon = dcon.copy()
        temp_dcon['na_obs_percentage'] = 1 # the number of observations that have missing values
        bdae_true_dataset = data.ImputationDatasetGen(config=temp_dcon, missing_vals=True, input_data=data_without_nas_2.data)
        bdae_true_train_loader = DataLoader(data.DatasetWithSplits(bdae_true_dataset, 'train', [0.8, 0.2, 0]), batch_size=mcon6['batch_size'], shuffle=True)
        bdae_true_val_loader = DataLoader(data.DatasetWithSplits(bdae_true_dataset, 'validation', [0.8, 0.2, 0]), batch_size=mcon6['batch_size'], shuffle=False)
        model_bdae_true = modelSDAE.SyntheticDenoisingAutoEncoder(noise_model=None, layer_dims_enc=mcon6['layer_dims_enc'], layer_dims_dec=mcon6['layer_dims_dec'], relu=mcon6['relu'], image=mcon6['image']).to(device)
        loss_fn_bdae_true = nn.MSELoss(reduction='none')
        optimizer_bdae_true = torch.optim.Adam(model_bdae_true.parameters(), lr=mcon6['learning_rate'])
        scheduler_bdae_true = StepLR(optimizer_bdae_true, step_size=mcon6['step_size'], gamma=mcon6['gamma'])
        print(model_bdae_true)
        helper_train_BDAE_true.train_imputation_model(model=model_bdae_true, loss_fn=loss_fn_bdae_true, optimizer=optimizer_bdae_true, scheduler=scheduler_bdae_true,
                                            dcon=dcon, mcon=mcon6, tcon=tcon3,
                                            train_dataloader=bdae_true_train_loader, validation_dataloader=bdae_true_val_loader, test_dataloader=na_test_loader_3,
                                            noise_model=mcon6['noise_model'])
        mse_all[4, i, cv_iteration-start_seed] = helper_train_BDAE_true.test(dataloader=na_test_loader_3, model=model_bdae_true, loss_fn=loss_fn_bdae_true, dcon=dcon, mcon=mcon6, tcon=tcon3)

    # save results to csv
    mse_this_run = torch.transpose(torch.transpose(mse_all, 0, 2), 1, 2)[cv_iteration-start_seed, :, :].cpu().numpy()

    mse_this_run_df = pd.DataFrame(mse_this_run,
                                   columns=['Patch_20', 'MNAR_20', 'MCAR_20'],
                                   index=['SDAE', 'SDAE_no_Enc', 'SDAE_zero_repl', 'BDAE', 'BDAE_true_dgp'])

    mse_this_run_df.to_csv(f'{folder_path}/mse_all_{cv_iteration}.csv')

# save overall results to Excel file
# import all the csv files from the results folder
results_mse = []
for file in os.listdir(folder_path):
    if file.endswith('.csv'):
        if 'mse_all' in file:
            results_mse.append(pd.read_csv(os.path.join(folder_path, file), header=0, index_col=0))
        else:
            raise ValueError('File name not supported')
#turn the list of dataframes into a numpy array with np.ndarray
results_mse = torch.tensor(np.array([df.values for df in results_mse]))
mean_rmse = torch.mean(torch.sqrt(results_mse), dim=0)
std_rmse = torch.std(torch.sqrt(results_mse), dim=0)

#save as xlsx files
with pd.ExcelWriter(f'{folder_path}/mean_results.xlsx') as writer:
    pd.DataFrame(mean_rmse).to_excel(writer, sheet_name='mean_rmse')
    pd.DataFrame(std_rmse).to_excel(writer, sheet_name='std_rmse')
